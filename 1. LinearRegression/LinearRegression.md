# 선형 회귀 모델 구현하기

## 선형회귀란?

### 선형 회귀: Linear Regression

- x 와 y값을 가지고 서로간의 관계를 파악하는것.
- 어떠한 입력에 대해서 출력값을 예측하는것.
- 머신러닝의 기본.
- 지도학습(Supervised Learning) 예측 문제에 사용하는 알고리즘.
- 선형회귀는 기본적으로 설명변수와 반응변수가 연속형 변수일 때 사용할수 있다.
- 만약 설명변수가 범주형 변수인 경우, 이를 더미변수 (Dummy Variable)로 변환하여 회귀분석을 적용해야 한다.

### 손실 함수: Loss Function

- 한 쌍(x, y)의 데이터에 대한 손실값을 계산하는 함수.
- 손실값이란 실제값과 모델로 예측한 값이 얼마나 차이가 나는가 이다.
- 즉 손실값이 작을수록 그 모델이 X 와 Y의 관계를 잘 설명하고 있다는 것.
- 이 손실을 전체 데이터에 대해 구한 경우 이를 비용(cost)이라고 한다.

손실 함수는 '예측값과 실제값의 거리'를 가장 많이 사용한다.
따라서 손실값은 예측값에서 실제값을 뺀뒤 제곱하여, 모든 데이터에 대한 평균을 내어 구한다.

이 손실함수를 만들때에는 잔차의 개념이 도입된다.

### 잔차: Residual

- 잔차란 관측값의 y와 예측값의 y 간의 차이를 말한다.
- 잔차 = hypothesis - y (여기서 hypothesis = W \* X + b, y는 예측값.)

```
A(1, 4) B(2, 3) 2개의점이 있다. 회귀식 = 2x + 1
A의 관측값 y는 4 예측값 y는 3
B의 관측값 y는 3, 예측값 y는 5

이때 A의 잔차는 4 - 3 = 1
B의 잔차는 3 - 5 = -2 이다
```

### 최소 제곱법: method of least squares, least squares approximation

최소 제곱법은 잔차의 제곱의 합이 최소가 되도록 하는 직선을 회귀선으로 한다는 것을 의미.

```
(hypothesis - 예측값) ** 2 -> 최소로 한다.
위에서 최소 제곱법으로 나온 값은 = (1) ** 2 + (-2) ** 2 = 5.
```

### 최적화 함수

- 가중치(W: Weight)와 편향(b: bias)값을 변경해가면서 손실값을 최소화 하는 가장 최적화된 가중치와 편향값을 찾아주는 함수.
- 값들을 무작위로 변경할시, 시간이 너무 오래걸리고, 학습 시간도 예측하기 어려울것.
- 따라서 빠르게 최적화 하기위한 다양한 방법 사용.

### 경사하강법: 최적화 방식중 하나

- 경사하강법은 최적화 방법중 가장 기본적인 알고리즘 으로, 함수의 기울기르루 구하고, 기울기가 낮은쪽으로 계속 이동시키면서 최적의 값을 찾아가는 방법
- Learning Rate, 즉 학습률은 학습을 얼마나 급하게 할것인가를 설정하는 값.
- 러닝 레이트가 너무 크면 최적의 손실갑슬 찾지 못하고 지나치게 되고, 너무 작으면 학습 속도가 매우 느려진다.

### 하이퍼 파라미터: hyperparameter

- 위의 Learning Rate처럼 학습에 영향을 주는 변수를 하이퍼 파라미터 라고 한다.
- 이 값에 따라 학습속도나 신경망 성능이 크게 달라진다.
